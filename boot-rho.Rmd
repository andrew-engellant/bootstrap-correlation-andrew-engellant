---
html_document:
  toc: true
  toc_depth: 6
  number_sections: true
  toc_float: true
  code_folding: hide
  theme: flatly
  code_download: true
author: "Andrew Engellant"
date: "`r format(Sys.time(), '%d %B, %Y')`"
title: "Bootstrap Correlation"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(scales)
library(here)
library(assertthat)
library(rsample)
```

## Introduction

In this assignment we explore how to test for significance when working with
the correlation coefficient. We'll calculate this statistic
in a manual fashion (something you'd _never_ 
do in real life). We'll then use bootstrapping to estimate standard errors 
of the statistics. We'll draw inference from those standard errors and compare
the outcome to the tests we're more familiar with. The data for this assignment
is described in an appendix at the bottom of this document. 

You might fairly ask why you need to calculate these statistics manually, when
there are fast and reliable functions that will give you the same value in a 
fraction of the time. We're mainly doing two things here: refreshing previous 
knowledge and learning the bootstrapping process. It's easy to take a stats
class that results in you vaguely knowing 
the $\chi^2$ statistic and correlation coefficient (symbolized by $\rho$). But 
writing the code from scratch usually locks in the ideas more firmly. 

With regard to the bootstrap process, you'll see that it's not too critical here, 
since the standard approaches to this analysis give the same results. 
In most cases the traditional approaches to estimating these quantities (seen in 
`chisq.test` and `cor.test`) are fine. Here the bootstrap is like squirrel hunting with
a bazooka. The power of the bootstrap is the ability to make good estimates of 
standard errors in situations where you have statistics that are _not_ well behaved
(or not well behaved with your population). See these statistics as a safe place
to learn how to code up something like this. I'm also hopeful that you might
be able to use a function like this in a case where the R function won't work with 
your data for some reason.

There's a secret third benefit as well. By writing code that matches the statistical
software, you're likely to trust your own abilities more. And the bootstrap 
method!

```{r data-input, message=F}
d <- read_tsv(paste(here(),"survey_data.txt",sep="/"))

```

## Exploring the Data

The correlation coefficient, $\rho$, measures the linear association between
two continuous variables. We can plot some of the numerical variables in our credit union
data set using the handy "scatterplot matrix" functionality of the package `car` that
accompanies the book "Companion to Applied Regression". This function plots the 
distribution of the data on the diagonals. The off-diagonal panels show the scatterplot
between the row variable and column variable. A smoothed curve and trendline are
both fit as well. 


```{r}
d %>%
  slice_sample(prop=0.2) %>% 
  select(age, account.age, progressivism, sustainability, localism) %>% 
  car::spm(pch=".")

```

We are interested in the degree of linear association between the two variables. For
instance, it appears there is a slight negative association between progressivism
and age. There appears to be strong positive association between sustainability and localism. 

To begin to get a sense of these data, let's look at a simple measure of association
(that, not coincidentally, is related to the mathematical definition of correlation). 
If two variables are correlated, than we would expect an above-mean value for one
variable to be associated with an above-mean value for the other. In the
code block below, calculate the following statistics: 

1. What fraction of observations in `d` have above average values for both
progessivism and age? 
1. What fraction of observations in `d` have above average values for
both sustainability and localism? 
1. What fraction of observations in `d` have above average values for
both sustainability and progressivism? 

After you've calculated the statistics, write a bit about what you expect
to happen when we use correlation coefficients to look at these relationships. 

```{r}
#Your code goes here
#proportion of observations with above average progressivism and age
mean(d$progressivism > mean(d$progressivism) &
       d$age > mean(d$age))

#proportion of observations with above average sustainability and localism
mean(d$sustainability > mean(d$sustainability) &
       d$localism > mean(d$localism))

#proportion of observations with above average susainability and progressivism
mean(d$sustainability > mean(d$sustainability) &
       d$progressivism > mean(d$progressivism))
```

I would expect the correlation coefficient to be furthest from zero when the proportion of observations with above average values in both variables is closest to 0.50. With this logic, I expect the coefficient for sustainability and localism to be greater than progressivism and age, and sustainability and progressivism.


## Correlation Coefficients

Recall that
the definition of the correlation between $X$ and $Y$ (two random variables) is
$$
\rho_{X,Y} = \frac{\textrm{cov}(X,Y)}{\sigma_X \cdot \sigma_Y}
$$

We can further expand this by writing out the definitions for covariance and standard
deviations: 

$$
\rho_{X,Y} = \frac{\textrm{cov}(X,Y)}{\sigma_X \cdot \sigma_Y} = 
  \frac{\sum(x_i - \bar{x})\cdot(y_i - \bar{y})}{\sqrt(
  \sum(x_i-\bar{x})^2 \cdot \sum (y_i-\bar{y})^2)}
$$

It may have been a while since you looked at this formula. Notice that the numerator
is the product of the x values minus the mean of the x values times the y values
minus their mean. Please answer this question as part of your assignment: 
When will the numerator of this fraction be large and when will it be small? 

The numerator will be smallest when the relationship between the two variables is more linear. A more linear relationship causes the positive and negative residuals to balance out and sum closer to zero between the two varibales. As the data becomes more spread, a greater proportion of observations fall either above or below their means, leading to a larger numerator.


The denominator, which is the product of the standard deviations, functions as
a normalizing term to ensure $-1 \leq \rho \leq 1$. 

Now let's build our own correlation coefficient. For the first step
of our bootstrap work, write a function called `my.cor` that takes as input 
two vectors and returns the correlation coefficient between them. Verify that 
you get the same results as the `cor` function on the age and progressivism
columns. Don't use `sd` or `cov` or, obviously, `cor`. 

```{r}
 # Finish your my.cor function here. 

my.cor <- function(x,y){
  num <- sum((x - mean(x)) * (y - mean(y)))
  den <- sqrt(sum((x - mean(x))**2) * sum((y - mean(y))**2))
  return(num/den) 
}

assert_that(abs(my.cor(d$age,d$progressivism)-cor(d$age,d$progressivism)) <= 0.0001)

```

Now that we've written and verified our function, we can use this to analyze
both the actual data set we have and a few thousand bootstrap replicates. The
latter will tell us what the standard error _around_ the correlation coefficient
is likely to be. Let's start with age and progressivism. 

```{r}

# store the actual value in the data
measured.val <- my.cor(d$age,d$progressivism)

# the number of simulations to do
n.sim <- 1000
set.seed(314159) # ensures repeatable results. 

# Create a data frame to hold our results. 
results <- tibble(statistic=rep(NA,n.sim))

```

Now, write code that fills the `results` data frame with bootstrap 
replicates. So you'll need to sample from `d` with replacement, then
calculate the correlation coefficient of age and progressivism on this new
data frame. Store those results by finishing the `for` loop, then I print out 
10 random values for us. We'll analyze and plot them down below. 

```{r}

for(i in 1:n.sim){
  # Resample with replacement
  sim <- d[sample(nrow(d), replace = TRUE), ]
  #Store correlation coefficients in results tibble
  results$statistic[i] <- my.cor(sim$age,sim$progressivism)
}

# now let's look at 10 random values. 
results %>% 
  slice_sample(n=10) %>%
  t() %>% 
  knitr::kable(digits=3)

```


Bootstrap resampling gives us estimates of the variability around an estimate. We
can use these replicates to get a confidence interval for our estimate of
`r round(measured.val,digits=3)` using the quantile function. A 90% confidence
interval ranges from `r round(quantile(results$statistic,0.05),digits=3)` to
`r round(quantile(results$statistic,0.95),digits=3)`. We can also plot the 
distribution of these values to understand the uncertainty in our estimate. 

```{r}
pnorm(0, mean(results$statistic), sd(results$statistic))
qnorm(.05, mean(results$statistic), sd(results$statistic))
quantile(results$statistic, p = .05)

ggplot(results,
       aes(x=statistic)) + 
  geom_density() + 
  theme_minimal() + 
  # Add vertical lines with `geom_vline`
  geom_vline(xintercept = measured.val, color = "red") +
  geom_vline(xintercept = quantile(results$statistic, p = .05), color = "grey") +
  geom_vline(xintercept = quantile(results$statistic, p = .95), color = "grey") +
  labs(x="Correlations from Bootstrap Replicates",
       y="Density") 


#Can we find the percentage of the distribution results below 0 to directly state our confidence that the correlation coefficient is below 0?
pnorm(0, mean(results$statistic), sd(results$statistic))
mean(results$statistic < 0)

#Can we be 100% confident the correlation is not 0? The odds of the true correlation coefficient being exactly zero seems nearly impossible.
mean(results$statistic != 0)
```

Add a red vertical line for `measured.val` and two gray vertical lines for the
5th and 95th percentiles of the distribution. Write a little bit about
what we can infer from the plot. Is the correlation likely to positive or negative?
How confident are we that the correlation is not zero? 

We are 90% confident that the true correlation is between -.0744 and -.0094. The proportion of simulated observations below zero was 0.983, indicating a 98.3% confidence that the correlation is negative. None of the simulated observations had a correlation of exactly zero, and we have very high confidence that the correlation is not zero. 



We can use base R's `cor.test` function as a comparison. 
```{r}
cor.test(d$progressivism,d$age,conf.level = 0.9)
```

The correlation test resulted in a correlation coefficient of -0.0418 which indicated progressiveness and age are weakly negatively correlated. This matches the correlation coefficient we previously calculated. The 90% confidence interval resulted in a range from -0.0751 and -0.0084. These values are very close to the ones calculated in our bootstrap simulation. If we increase the number of simulations from 1000 to 10000 or 100000 our calculated confidence interval would likely match the cor.test results with higher precision. 
<!-- Briefly interpret this test around here. How does the confidence interval
     compare to what we calculated via bootstrap replication? --> 

Now repeat the above analyses for our other two correlations, sustainability versus
localism and sustainability versus progressivism. Repeat the process from above,
doing bootstrap replicates, calculating confidence intervals, plotting the results,
and interpreting them. As above, compare your results to `cor.test`. 

### Additional Correlations 

##Sustainability and Localism Correlation

```{r}

# store the actual value in the data
measured.val <- my.cor(d$sustainability,d$localism)

# the number of simulations to do
n.sim <- 1000
set.seed(314159) # ensures repeatable results. 

# Create a data frame to hold our results. 
results <- tibble(statistic=rep(NA,n.sim))

#Bootstrap Resample
for(i in 1:n.sim){
  sim <- d[sample(nrow(d), replace = TRUE), ]
  results$statistic[i] <- my.cor(sim$sustainability,sim$localism)
}

# now let's look at 10 random values. 
results %>% 
  slice_sample(n=10) %>%
  t() %>% 
  knitr::kable(digits=3)

ggplot(results,
       aes(x=statistic)) + 
  geom_density() + 
  theme_minimal() + 
  # Add vertical lines with `geom_vline`
  geom_vline(xintercept = measured.val, color = "red") +
  geom_vline(xintercept = quantile(results$statistic, p = .05), color = "grey") +
  geom_vline(xintercept = quantile(results$statistic, p = .95), color = "grey") +
  labs(x="Correlations from Bootstrap Replicates",
       y="Density") 

quantile(results$statistic, p = .05)
quantile(results$statistic, p = .95)

cor.test(d$sustainability, d$localism, conf.level = 0.9)
```

The correlation coefficient from the sampled data for sustainability and localism is 0.496 indicating a moderate positive correlation between the two variables. We have 90% confidence that the actual correlation for these two variable is between 0.467 and 0.526. This confidence interval is very close to the values produced by the cor.test function, which give a 90% confidence interval of 0.470 to 0.521.

##Sustainability and Progressivism Correlation

```{r}

# store the actual value in the data
measured.val <- my.cor(d$sustainability,d$progressivism)

# the number of simulations to do
n.sim <- 1000
set.seed(314159) # ensures repeatable results. 

# Create a data frame to hold our results. 
results <- tibble(statistic=rep(NA,n.sim))

#Bootstrap Resample
for(i in 1:n.sim){
  sim <- d[sample(nrow(d), replace = TRUE), ]
  results$statistic[i] <- my.cor(sim$sustainability,sim$progressivism)
}

# now let's look at 10 random values. 
results %>% 
  slice_sample(n=10) %>%
  t() %>% 
  knitr::kable(digits=3)

ggplot(results,
       aes(x=statistic)) + 
  geom_density() + 
  theme_minimal() + 
  # Add vertical lines with `geom_vline`
  geom_vline(xintercept = measured.val, color = "red") +
  geom_vline(xintercept = quantile(results$statistic, p = .05), color = "grey") +
  geom_vline(xintercept = quantile(results$statistic, p = .95), color = "grey") +
  labs(x="Correlations from Bootstrap Replicates",
       y="Density") 

quantile(results$statistic, p = .05)
quantile(results$statistic, p = .95)

cor.test(d$sustainability, d$progressivism, conf.level = 0.9)
```

The correlation coefficient from the sampled data for sustainability and progressivism is 0.372 indicating a moderate positive correlation between the two variables. We have 90% confidence that the actual correlation for these two variable is between 0.342 and 0.403. This confidence interval is very close to the values produced by the cor.test function, which give a 90% confidence interval of 0.342 to 0.400. 


## Appendix: Full Data Description
A financial institution in Washington has become concerned that their current membership base is not well-aligned with their corporate values. Through that concern they realized that don't actually understand their membership's values very well. They surveyed 2,421 members to shed light on the issue. 

The heart of the survey was the Moral Foundations Theory of Jonathan Haidt. Members were surveyed on the Moral Foundations Questionnaire, which you should take so you understand the test. Survey respondents were scored on the five foundations as well as a single-number summary, Progressivism. 

The financial institution values Localism, Sustainability, and Education. These aspects of member's values were assessed in the survey as well. Localism and Sustainability used validated scales and thus can be summarized via a single score, where higher values indicate greater support for the values. Education is summarized by the following three questions, which we do not have evidence can be combined into a single score:

* In general, public schools provide a better education than private schools.
* Public school teachers are underpaid.
* Experience is more important than education in determining success in life.
These questions were evaluated on a 1 to 6 scale where 1 indicated "Strongly Disagree" and 6 indicated "Strongly Agree". 

Finally, we have information on the member that can be used to understand variation in their values. 

The data consists of the following columns:

* ID: a unique identifier for the survey respondent.
* age: the age of the respondent.
* gender: gender was evaluated with robust scale and collapsed into male/female/other for those whose gender identity was not male or female.
* engagement: three categories of engagement with the financial institution.
* mem.edu: the self-reported education level of the member with the following scale:
* zip: the member zip code. 
* channel: how the member joined the financial institution. Options are "Loan" if they joined via an auto loan, "Branch" if they joined at a branch and other for online or unknown. 
* progressivism/harm/fair/in.group/authority/purity: The MFQ results.
* account.age: the age of the member's account, in years. 
* region: The region of Washington the member lives in. May be easier to work with than zip.
* public.sector: has the person ever been a public employee?
* sustainability/localism: Scores on the validated scales. Higher values indicate greater support for the value.
* pub.greater.priv/experience.more.important/teachers.underpaid: The responses to the education questions above. 
* main.focal.value: Respondents were asked, "Below is a list of broad areas to which people often dedicate their volunteer or philanthropic efforts. From this list, please select the most important to you. If an area of particular importance is missing, please let us know about it in the space for 'other.'" This column holds the respondents' answer to that question. 
* support.of.focal.value: Respondents were given an opportunity to indicate how they supported their focal value. Those responses were collapsed into a single score, where a higher value indicates more support.










